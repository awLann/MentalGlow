# -*- coding: utf-8 -*-
"""Mental_Health_Classifier 0.90.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pZVHGZQRjpZITJCrRp92x4vQgE7mTh3L

# Importing
"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

import re
import string
from wordcloud import WordCloud
from collections import Counter

import warnings
warnings.filterwarnings('ignore')
!pip install scikit-plot
from nltk import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D # deep learning techniques
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer

from google.colab import drive
drive.mount('/content/drive')

"""# Data Outline

**1) The first 5 lines in data**
"""

import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/BANGKIT/projek/mental_health.csv')
data.head()

!pip install googletrans==3.1.0a0
# Function to translate text from English to Indonesian
from googletrans import Translator

# init the Google API translator
def translate_to_indonesian(text):
    translator = Translator()
    try:
        # use translator.translate() function to translate text
        translation = translator.translate(text, src='en', dest='id')
        # return the translated text
        return translation.text
    except Exception as e:
        # return an error message if there's an error in translation
        return "Error translating text: {}".format(str(e))
# init the Google API translator
translator = Translator()

# set the timeout parameter to 10 seconds
translation = translator.translate('Hello, world!', src='en', dest='id', timeout=10)

# print the translated text
print(translation.text)

# import pandas library
import pandas as pd

# Apply the translation function to the 'text' column
data['text_indo'] = data['text'].apply(translate_to_indonesian)

# Save the translated DataFrame to a CSV file
data.to_csv('data_terjemahan.csv', index=False)

# Display the DataFrame to check the results
print(data[['text', 'text_indo']])

import pandas as pd
data = pd.read_csv("/content/drive/MyDrive/BANGKIT/projek/data_terjemahan.csv")

"""**2) Data shape**"""

print(data.shape)

"""**3) Null in data**"""

data.isnull().sum()

"""**4) The number by labels**"""

data['label'].value_counts()

"""**5) Bar and Pie plot by '0' and '1'**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

labels = [0, 1]
sizes = [14139, 13838]
custom_colours = ['#ff7675', '#74b9ff']

plt.figure(figsize=(20, 6), dpi=227)
plt.subplot(1, 2, 1)
plt.pie(sizes, labels = labels, textprops={'fontsize': 15}, startangle=140,
       autopct='%1.0f%%', colors=custom_colours, explode=[0, 0.05])

plt.subplot(1, 2, 2)
sns.barplot(x = data['label'].unique(), y = data['label'].value_counts(), palette= 'viridis')

plt.show()

"""**6) Creating new columns 'Total Words' and 'Total Chars'**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

data['Total Words'] = data['text_indo'].apply(lambda x: len(x.split()))

def count_total_words(text):
    char = 0
    for word in text.split():
        char += len(word)
    return char

data['Total Chars'] = data["text_indo"].apply(count_total_words)

data.describe()

"""**7) Kdeplot by Total Words**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

plt.figure(figsize = (10, 6))
sns.kdeplot(x = data['Total Words'], hue= data['label'], palette= 'winter', shade = True)
plt.show()

"""**8) Kdeplot by Total Chars**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

plt.figure(figsize = (10, 6))
sns.kdeplot(x = data['Total Chars'], hue= data['label'], palette= 'winter', shade = True)
plt.show()

"""# Text preprocessing

**1) Lowercasing**
"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

def convert_lowercase(text):
    text = text.lower()
    return text

data['text_indo'] = data['text_indo'].apply(convert_lowercase)

"""**2) Removing URLs**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

def remove_url(text):
    re_url = re.compile(r'http\S+|www\S+|https\S+')
    return re_url.sub('', text)

data['text_indo'] = data['text_indo'].apply(remove_url)

"""**3) Removing Punctuations (Tanda Baca)**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

exclude = string.punctuation

def remove_punc(text):
    return text.translate(str.maketrans('', '', exclude))

data['text_indo'] = data['text_indo'].apply(remove_punc)
data

import nltk
nltk.download('punkt')
nltk.download('stopwords')
data['tokens'] = data['text_indo'].astype(str)
data['tokens'] = data['tokens'].apply(word_tokenize)

data['tokens'] = data['tokens'].apply(lambda x: [token for token in x if 4 <= len(token) <= 22])
data

"""**4) Removing stopwords (kata tidak relevan)**"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
stopword = nltk.corpus.stopwords.words('indonesian')

def remove_stopwords(text):
  text = [word for word in text if word not in stopword]
  return text

data['tokens'] = data['tokens'].apply(lambda x: remove_stopwords(x))

"""**5) Stemming**"""

# Stemming
import re

def stem_word(word):
    # Remove inflection suffixes
    word = re.sub(r'(-lah|-ku|-nya|-mu|-kah|-tah|-pun)$', '', word)

    # Remove derivation suffixes
    word = re.sub(r'(-kan|-i|-an)$', '', word)

    # If "-an" has been removed and the word ends with "k", remove "k"
    if word.endswith('k'):
        word = word[:-1]

    return word

data['tokens'] = data['tokens'].apply(lambda x: [stem_word(token) for token in x])


# Join the tokens back into a string
data['processed_text'] = data['tokens'].apply(' '.join)

data

data.to_csv("data text.csv")

"""# Data Visualization

**1) Word Cloud by label '0'**
"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

text = " ".join(data[data['label'] == 0]['text_indo'])
plt.figure(figsize = (15, 10))
wordcloud = WordCloud(max_words=500, height= 800, width = 1500,  background_color="black", colormap= 'viridis').generate(text)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""**2) Word Cloud by label '1'**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

text = " ".join(data[data['label'] == 1]['text_indo'])
plt.figure(figsize = (15, 10))
wordcloud = WordCloud(max_words=500, height= 800, width = 1500,  background_color="black", colormap= 'viridis').generate(text)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""**3) Word Frequency by label '0'**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

all_spam_words = []
for sentence in data[data['label'] == 0]['text_indo'].to_list():
    for word in sentence.split():
        all_spam_words.append(word)

df = pd.DataFrame(Counter(all_spam_words).most_common(25), columns= ['Word', 'Frequency'])

sns.set_context('notebook', font_scale= 1.3)
plt.figure(figsize=(18,8))
sns.barplot(y = df['Word'], x= df['Frequency'], palette= 'summer')
plt.title("Most Commonly Used Words")
plt.xlabel("Frequnecy")
plt.ylabel("Words")
plt.show()

"""**4) Word Frequency by label '1'**"""

# code by "Spam Classifier | NLP | 98% Accuracy" by ANUBHAV GOYAL( https://www.kaggle.com/code/sasakitetsuya/spam-nlp-98-accuracy )

all_spam_words = []
for sentence in data[data['label'] == 1]['text_indo'].to_list():
    for word in sentence.split():
        all_spam_words.append(word)

df = pd.DataFrame(Counter(all_spam_words).most_common(25), columns= ['Word', 'Frequency'])

sns.set_context('notebook', font_scale= 1.3)
plt.figure(figsize=(18,8))
sns.barplot(y = df['Word'], x= df['Frequency'], palette= 'summer')
plt.title("Most Commonly Used Words")
plt.xlabel("Frequnecy")
plt.ylabel("Words")
plt.show()

"""from sklearn.feature_extraction.text import TfidfVectorizer# Modeling"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
# Pisahkan fitur (teks) dan label
X = data['processed_text'].values
Y = data['label'].values

# Parameter untuk Tokenizer dan padding
max_words = 10000
max_len = 200

# Tokenizer
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X)

# Mengubah teks menjadi urutan angka
sequences = tokenizer.texts_to_sequences(X)

# Padding sequences untuk panjang yang sama
data_pad = pad_sequences(sequences, maxlen=max_len)

# Pembagian data secara manual
X_train = data_pad[:22381]
Y_train = Y[:22381]
X_test = data_pad[22382:]
Y_test = Y[22382:]

# Konversi label ke numpy array dengan tipe float
Y_train = np.array(Y_train, dtype=float)
Y_test = np.array(Y_test, dtype=float)

# Konversi data input dan output ke tipe data yang benar
X_train = X_train.astype(np.int32)
X_test = X_test.astype(np.int32)
Y_train = Y_train.astype(np.float32)
Y_test = Y_test.astype(np.float32)

# Verifikasi tipe data
print(f"Input data type: {X_train.dtype}, {X_test.dtype}")
print(f"Output data type: {Y_train.dtype}, {Y_test.dtype}")

# Definisi model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=max_words, output_dim=16, input_length=max_len),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Kompilasi model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Tampilkan ringkasan model
model.summary()

# Latih model
history = model.fit(X_train, Y_train, epochs=30, batch_size=64, validation_split=0.2)
# Evaluasi model pada data pengujian
loss, accuracy = model.evaluate(X_test, Y_test)
print(f"Test Accuracy: {accuracy:.4f}")
# Simpan model dalam format H5
model.save("my_model.h5")

# Prediksi dengan model
predictions = model.predict(X_test)

# Konversi probabilitas prediksi menjadi label biner (0 atau 1)
predicted_labels = (predictions > 0.5).astype(int)

# Hitung akurasi keseluruhan
overall_accuracy = np.mean(predicted_labels == Y_test.reshape(-1, 1))
print(f"Overall Accuracy: {overall_accuracy:.4f}")

# Output prediksi untuk beberapa contoh
for i in range(len(predicted_labels)):
    print(f"Text: {X[i]} - Predicted Probability: {predictions[i][0]:.4f} - Predicted Label: {predicted_labels[i][0]} - Actual Label: {Y_test[i]}")

plt.figure(figsize=(10,5))
plt.style.use('dark_background')
plt.plot(history.history['loss'])
plt.plot(history.history['accuracy'])

plt.title('Model loss')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc = 'upper right')
plt.show()

import matplotlib.pyplot as plt

# Plot loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Hitung metrik-metrik evaluasi
from sklearn.metrics import classification_report
report = classification_report(Y_test, predicted_labels, target_names=['0', '1'])

# Tampilkan laporan evaluasi
print(report)

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X)

tokenizer.word_index

import re
import string
import nltk
from nltk.tokenize import word_tokenize
from tensorflow.keras.models import load_model
model = load_model("/content/my_model.h5")
def stem_word(word):
    # Remove inflection suffixes
    word = re.sub(r'(-lah|-ku|-nya|-mu|-kah|-tah|-pun)$', '', word)

    # Remove derivation suffixes
    word = re.sub(r'(-kan|-i|-an)$', '', word)

    # If "-an" has been removed and the word ends with "k", remove "k"
    if word.endswith('k'):
        word = word[:-1]

    return word
def preprocess_text(text):
    # Lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize
    tokens = word_tokenize(text)

    # Remove stopwords
    stopword = nltk.corpus.stopwords.words('indonesian')
    tokens = [word for word in tokens if word not in stopword]

    # Stemming
    tokens = [stem_word(token) for token in tokens]

    # Join tokens back to string
    processed_text = ' '.join(tokens)

    return processed_text

# Inisialisasi tokenizer
max_words = 10000
max_len = 200
#tokenizer = Tokenizer(num_words=max_words)

# Definisikan fungsi prediksi kelas
def predict_class(model, tokenizer, text):
    # Memuat kelas label (gantilah dengan kelas label Anda)
    list_labels = ["0", "1"]

    # Tokenisasi teks
    sequences = tokenizer.texts_to_sequences([text])
    print(sequences)
    # Padding sequences untuk panjang yang sama
    data_pad = pad_sequences(sequences, maxlen=max_len)
    # Prediksi dengan model
    prediction = model.predict(data_pad)[0]

    # Ambil indeks kelas dengan probabilitas tertinggi
    predicted_index = np.argmax(prediction)
    # Buat dictionary hasil prediksi
    result = {list_labels[predicted_index]: prediction[predicted_index]}
    return (prediction > 0.5).astype(int)[0],prediction[0]

# Contoh teks yang ingin Anda prediksi
text_to_predict = "lebih baik pergi daripada hidup sendiri sudah capek apalagi orang tua sangat mendesak"

# Preprocess teks
preprocessed_text = preprocess_text(text_to_predict)

# Prediksi kelas
prediction_result = predict_class(model, tokenizer, preprocessed_text)
print(prediction_result)

import pickle
# Simpan tokenizer
tokenizer_path = "save_tokenizer.pkl"
with open(tokenizer_path, "wb") as f:
    pickle.dump(tokenizer, f)

# Kemudian, saat Anda memuat kembali tokenizer:
with open(tokenizer_path, "rb") as f:
    tokenizer = pickle.load(f)

import tensorflow as tf

# Load model
model = tf.keras.models.load_model("my_model.h5")

# Konversi model ke format TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Simpan model TensorFlow Lite ke file
with open('my_model.tflite', 'wb') as f:
    f.write(tflite_model)

import numpy as np
import tensorflow as tf

# Load model
interpreter = tf.lite.Interpreter(model_path="/content/my_model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
list_labels = list(np.array([0, 1]))
# Define function to predict class
def predict_class(interpreter, tokenizer, text):
    # Tokenize text
    sequences = tokenizer.texts_to_sequences([text])
    data_pad = pad_sequences(sequences, maxlen=max_len)

    # Set input tensor
    input_data = np.array(data_pad, dtype=np.float32)
    interpreter.set_tensor(input_details[0]['index'], input_data)

    # Run inference
    interpreter.invoke()

    # Get output tensor
    output_data = interpreter.get_tensor(output_details[0]['index'])

    # Extract prediction
    prediction = output_data[0]
    predicted_index = np.argmax(prediction)
    result = {list_labels[predicted_index]: prediction[predicted_index]}
    return result

# Example text to predict
text_to_predict = 'ingin laut ingin menenggelamkan orangtemanku ingin bunuh diri melelahkan ya butuh bantuan dukungan selalu mengharapkan mendapatkannya bicara orang tua dapatkan terapis menelepon hotline dump sialan setiap kali benar-benar teman yang akan mendengarkan aku mulai mengerti mengapa perawatan tidak bisa menangani praktis minggu kelelahan bahkan seharusnya katakan padanya semuanya mengeluh benar ya seseorang suka seseorang peduli apakah hidup mati pengecualian orang mungkin ya gendut jelek ya tidak mampu menjaga bahkan bodoh upah minimal pekerjaan ya gagal sekolah ya hidup sia-sia tak berarti tidak ada apa pun yang mengirimkan foto pistol memberitahu otak akan dinding membuat merasa seperti sandera tak berdaya takut pergi ingin bicara dengarkan rekaman rusak cerita malam sampai dia cukup mabuk pingsan jam tidur mampu dia menganggur memberikan sekolah sialan aku tangan fungsi bangun dengan baik jam tidur semua berpikir aku mencapai titik segera kelelahan lelah kurang tidur katakan pergi itu hampir yakin akan pilihan terbaik dia apa pun memperbaiki situasi mengubah apa pun Bikin depresi tak peduli lagi mengikis tetap kewarasan'

# Preprocess text
preprocessed_text = preprocess_text(text_to_predict)

# Predict class
prediction_result = predict_class(interpreter, tokenizer, preprocessed_text)
print(prediction_result)

!pip install tensorflowjs

!tensorflowjs_converter --input_format=keras '/content/my_model.h5' my_dir